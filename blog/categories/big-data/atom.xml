<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big Data | To Boldly Code...]]></title>
  <link href="http://boldlycoding.com/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://boldlycoding.com/"/>
  <updated>2015-03-18T02:58:09-04:00</updated>
  <id>http://boldlycoding.com/</id>
  <author>
    <name><![CDATA[George McDaid]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Collecting and Processing Tweets]]></title>
    <link href="http://boldlycoding.com/blog/2015/03/17/collecting-and-processing-tweets/"/>
    <updated>2015-03-17T17:55:55-04:00</updated>
    <id>http://boldlycoding.com/blog/2015/03/17/collecting-and-processing-tweets</id>
    <content type="html"><![CDATA[<!-- more -->


<p><img class="center" src="/images/tweet_process.png" width="1000" height="400" title="Tweet collection and processing" ></p>

<h2>Collecting the Tweets</h2>

<p>Tweet collection is driven the Apache project Flume. Flume collects, aggregates, and moves variable amounts of streaming data. The data can be from a variety of sources including server and other logs. Flume overall is very flexible and customisable based on the particular application. It not only support multiple sources, but also multiple processing mediums, or channels. Here channels are where the data in question is stored before being being written to its final destination, or sink. Flume captures data from a source, and stores it in a channel before finally writing the data to the sink. Channels can be in memory, in a file, or files, or a combination of the two. The logical structure of the source, channel and sink, is an agent. A flume agent that captures web server data and writes it to a Hadoop HDFs would logically look like the following</p>

<p><img class="center" src="/images/flume_agent.png" width="540" height="440" title="A typical flume agent" ></p>

<p>As discussed in a previous post, I have installed Cloudera&rsquo;s CDH in my cluster. The CDH allows for convenient definition of Flume Agent. After selecting the Flume service and clicking the Configuration tab, property Configuration File can be searched for. My agent definition was</p>

<pre><code>TwitterAgent.sources = Twitter
TwitterAgent.channels = MemChannel
TwitterAgent.sinks = HDFS

TwitterAgent.sources.Twitter.type = com.boldlycoding.twitterstreaming.TrendingStream
TwitterAgent.sources.Twitter.channels = MemChannel

TwitterAgent.sinks.HDFS.channel = MemChannel
TwitterAgent.sinks.HDFS.type = hdfs
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://typhlosion:8020/user/flume/tweets/%Y/%m/%d/%H/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text
TwitterAgent.sinks.HDFS.hdfs.batchSize = 80
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
TwitterAgent.sinks.HDFS.hdfs.rollCount = 100
TwitterAgent.sinks.HDFS.hdfs.rollInterval = 120
TwitterAgent.sinks.HDFS.hdfs.minBlockReplicas = 1

TwitterAgent.channels.MemChannel.type = memory
TwitterAgent.channels.MemChannel.capacity = 10000
TwitterAgent.channels.MemChannel.transactionCapacity = 100
</code></pre>

<p>This Flume agent definition uses a custom flume source defined in my uploaded jar. The channel is a Memory Channel, meaning all the tweets will be placed in memory before being written to the sink, which here is the Hadoop HDFS. All of the tweets are written in a text format. The agent also writes 100 tweets to each file before starting a new file, and will automatically close a file after 2 minutes even if 100 tweets have not been written. The memory channel is defined with a capacity of 10,000 tweets and 100 tweets will be move in/out of the channel at a time. The HDFS path parameter defines where on the HDFS the files will be written and the percent signs and characters at the end of the path indicate the directories will be created based on the timestamp of each tweet.</p>

<p>To do the actual capturing, processing, and dispatching to the memory channel, I have defined a custom Flume source by extending the Flume abstract source, access credentials withheld.</p>

<pre><code>public class TrendingStream extends AbstractSource implements EventDrivenSource, Configurable {

    Authentication auth;

    @Override
    public void start() {
        final ChannelProcessor channel = getChannelProcessor();
        final BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;String&gt;(10000);
        StatusesFilterEndpoint endpoint = new StatusesFilterEndpoint();
        ArrayList&lt;String&gt; filter_trends = Lists.newArrayList("college", "university", "uni", "school", "higher education", "grad school");
        endpoint.trackTerms(filter_trends);
        Client client = new ClientBuilder()
                .hosts(Constants.STREAM_HOST)
                .endpoint(endpoint)
                .authentication(auth)
                .processor(new StringDelimitedProcessor(queue))
                .build();
        client.connect();
        final JsonParser jsonParser = new JsonParser();
        final Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;();

        TwitterThread tweam = new TwitterThread() {

            @Override
            public void run() {
                for (int msgRead = 0; msgRead &lt; 1000; msgRead++) {
                    try {
                        String msg = queue.take();
                        msg = msg.replace("\n", "").replace("\r", "").replace("\\n", "");
                        msg = msg.replaceAll("\\\\\"", "");
                        msg = msg.replaceAll("\\\\","");
                        msg = msg.replaceAll("/","");
                        //System.out.println(msg);
                        JsonObject user = jsonParser.parse(msg)
                                .getAsJsonObject().get("user")
                                .getAsJsonObject();
                        String zone = user.get("time_zone").getAsString();
                        //System.out.println(zone);
                        msg = msg.trim();
                        headers.put("timestamp", String.valueOf(new Date().getTime()));
                    Event event = EventBuilder.withBody(msg.getBytes(), headers);
                    channel.processEvent(event);
                    } catch (InterruptedException ex) {
                        Logger.getLogger(TrendingStream.class.getName()).log(Level.SEVERE, null, ex);
                    } catch (UnsupportedOperationException ex) {
                        msgRead--;
                    } catch (NullPointerException ex) {
                        msgRead--;
                        try {
                            Thread.sleep(2000);
                        } catch (InterruptedException ex1) {
                            System.out.println("Thread Interrupt");
                        }
                    }

                    if (msgRead &lt;= -10000) {
                        break;
                    }

                }
            }
        };

        tweam.run();

        super.start();

    }

    @Override
    public void configure(Context cntxt) {
        auth = new OAuth1("consumerKey", "consumerSecret", "token", "tokenSecret");
    }

    @Override
    public void stop() {
        super.stop();
    }
}
</code></pre>

<p>The source begins by defining a Flume channel processor, blocking queue, and a filtering status endpoint. The processor and queue and more obvious. The filtering status endpoint captures live tweets, and applies a set of filtering terms. A tweets must contain all the terms in one of the items in the set. For instance, referencing the set in the previous code snippet, a tweet with the work college would be captured. A tweet containing higher education would be captured as well, but a tweet only containing the word higher or education would not.</p>

<p>In this code snippet the loop would capture 1000 tweets before stopping. When a tweet is captured the code filters out backslashes, double backslashes, forward slashes, and new lines from the JSON data format of the tweet. This is important because if they are not filtered out it causes the JSON parser to throw and error and is unable to process the tweets. This was one of the more difficult points to discover, and filtering out all the necessary characters was an extremely iterative process. After the tweet is cleaned, the tweet is checked for a valid (non-blank) time zone which allows for regionalization of the sentiment data (It is interesting to note that since Twitter is built on Ruby all of the timezone options are the same as those that are found in Ruby). Through the course of the study I have noted that about half of tweets have an invalid time zone. If a valid time zone is not found, the loop is set back an iteration. This allows for the specified number of tweets to be captured. Also, to avoid an infinite loop the loop control variable is check for a large negative value, and a break is issued if a large value is found. This should be relative to the number of tweets that need to be captured. Finally, the tweet data is stamped with a timestamp and it is passed to the channel processor. The settings in the Flume Agent configuration then control how the channel handles placing the events into the sink. This was another very difficult part of the project to configure as the channel must be set up so it is not overwhelmed by the speed of incoming tweets.</p>

<h2>Processing the Tweets</h2>

<p>To process the sentiment data in the Hadoop HDFS Hive (provided with Cloudera CDH) is used. Hive is an Apache project that provides a SQL interface for querying the Hadoop HDFS. The run the SQL queries, Hive runs MapReduce jobs taking advantage of the other distributed Hadoop component. The following Hive script analyzes the Tweet data and produces several new tables the last of which contains the required sentiment data.</p>

<pre><code>DROP TABLE tweets_raw;
DROP TABLE dictionary;
DROP TABLE time_zone_map;
DROP VIEW tweets_simple;
DROP VIEW l1;
DROP VIEW l2;
DROP VIEW l3;
DROP TABLE tweets_sentiment;
DROP TABLE tweetsbi;

CREATE EXTERNAL TABLE tweets_raw (
   id BIGINT,
   created_at STRING,
   source STRING,
   favorited BOOLEAN,
   retweet_count INT,
   retweeted_status STRUCT&lt;
      text:STRING,
      user:STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
   entities STRUCT&lt;
      urls:ARRAY&lt;STRUCT&lt;expanded_url:STRING&gt;&gt;,
      user_mentions:ARRAY&lt;STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
      hashtags:ARRAY&lt;STRUCT&lt;text:STRING&gt;&gt;&gt;,
   text STRING,
   user STRUCT&lt;
      screen_name:STRING,
      name:STRING,
      friends_count:INT,
      followers_count:INT,
      statuses_count:INT,
      verified:BOOLEAN,
      utc_offset:STRING, -- was INT but nulls are strings
      time_zone:STRING&gt;
)
ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe'
LOCATION '/user/flume/tweets'
;

-- create sentiment dictionary
CREATE EXTERNAL TABLE dictionary (
    type string,
    length int,
    word string,
    pos string,
    stemmed string,
    polarity string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
STORED AS TEXTFILE
LOCATION '/user/home/gmcdaid/data/dictionary';

CREATE EXTERNAL TABLE time_zone_map (
    time_zone string,
    country string,
    notes string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
STORED AS TEXTFILE
LOCATION '/user/home/gmcdaid/data/time_zone_map';

CREATE VIEW tweets_simple AS
SELECT
  id,
  cast ( from_unixtime( unix_timestamp(concat( '2015 ', substring(created_at,5,15)), 'yyyy MMM dd hh:mm:ss')) as timestamp) ts,
  text,
  time_zone_map.country
FROM tweets_raw
JOIN time_zone_map ON tweets_raw.user.time_zone = time_zone_map.time_zone
;

create view l1 as select id, words from tweets_simple lateral view explode(sentences(lower(text))) dummy as words;
create view l2 as select id, word from l1 lateral view explode( words ) dummy as word ;

create view l3 as select 
    id, 
    l2.word, 
    case d.polarity 
      when  'negative' then -1
      when 'positive' then 1 
      else 0 end as polarity 
 from l2 left outer join dictionary d on l2.word = d.word;

create table tweets_sentiment stored as orc as select 
id, 
case 
when sum( polarity ) &gt; 0 then 'positive' 
when sum( polarity ) &lt; 0 then 'negative'  
else 'neutral' end as sentiment 
from l3 group by id;

CREATE TABLE tweetsbi 
STORED AS ORC
AS
SELECT 
  t.*,
  case s.sentiment 
    when 'positive' then 2 
    when 'neutral' then 1 
    when 'negative' then 0 
  end as sentiment  
FROM tweets_simple t JOIN tweets_sentiment s on t.id = s.id;
</code></pre>

<p>This script has a multitude of intermediate steps. It begins by creating a table containing all the raw Twitter information. The accommodate all the nested structures in the JSON Hive provides ARRAY and STRUCT structures. The table specifies a SERDE and LOCATION, the location leaves all the data on the file system instead of importing it. The SERDE clause instructs the table to parse the data using the specified serializer/deseralizer. This is a generic piece of code provided by Cloudera to process JSON data. Next, the script creates the dictionary and time zone tables. The dictionary contains all of the words to be analysed and their associated sentiment values. This is sometimes referred to as a lexicon. The time zone table maps the twitter time zones to generic regions. The list was created using the list of time zones that Ruby supports. After this, the script creates a simpler tweets table containing only the required information. Following this three lateral views are created to analyse the twitter data using the information from the dictionary file. The final two steps reorganize the data and create more readable tables. Positive sentiment becomes a 2, neutral sentiment is a 1, and negative sentiment is a 0. Following processing all of the tweet data, the data can be presented, in a later post I will discuss using PHP and Google charts to present the data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Twitter Trends]]></title>
    <link href="http://boldlycoding.com/blog/2015/02/11/twitter-trends/"/>
    <updated>2015-02-11T01:27:08-05:00</updated>
    <id>http://boldlycoding.com/blog/2015/02/11/twitter-trends</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Twitter defines trends as those topics which are popular now, not topics which have been popular for a long period of time. During the course of my project thus far I have been tracking trends and making observations about them. One interesting observation about Twitter trends is to volatility of the trends. Trends change on a frequent basis, which suggests an instability of conversation on Twitter. It is also interesting to note the
number of trends which are not of appreciable social, political or economic significance. From January to February 2015 trends included #ReplaceAMovieTitleWithGoat, #HappyBirthdayHarryStyles, #NationalPizzaDay, #RuinAFriendshipIn5Words and a number of other One Direction topics. The instability of trends and lack of politically, socially and economically trends does pose a problem for my project. To cope with this problem I have begun tracking trends every two minutes and reviewing the trends periodically. If a significant topic does begin to trends. I will be able to start tracking that trends and then analyse the related tweets. Twitter trends are also based on location. Twitter will tailor the trends displayed based on the location of the user. My project will focus on worldwide opinions and topics so this posed another temporary problem. The Twitter API allows for the acquisition of trends on a global scale. This list of trends is not influenced by any particular user&rsquo;s information or location. Overall, Trends tend to be very complex and will continue to shape the project towards it completion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing Cloudera Manager]]></title>
    <link href="http://boldlycoding.com/blog/2014/12/29/installing-cloudera-manager/"/>
    <updated>2014-12-29T08:47:51-05:00</updated>
    <id>http://boldlycoding.com/blog/2014/12/29/installing-cloudera-manager</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Cloudera Manager (CM) will install the Oracle JDK, CDH (runs Hadoop), and other service management software. The manager administrates many services including HDFS, YARN, ZooKeeper, Oozie, Hive, Hue, Sqoop, HBase, Impala, Solr and Spark. Many of these can be installed only if they are needed. The CM installation process includes a section to select all or a subset of the services.</p>

<ol>
<li><p>Start a tmux session with</p>

<pre><code class="`"> tmux
</code></pre>

<p> In the event of a disconnect reconnect to the machine and run</p>

<pre><code class="`"> tmux a
</code></pre>

<p> Tmux will try to attach to a previously running session.</p></li>
<li><p>Download and start the Cloudera Manager installer on the cluster manager</p>

<pre><code class="`"> wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin
 chmod u+x cloudera-manager-installer.bin
 sudo ./cloudera-manager-installer.bin
</code></pre></li>
<li><p>If the installation was started correctly, it should display what is shown in the screenshot below. Select <em>Next</em> to continue.</p>

<p> <img class="center" src="/images/cm/3_start.PNG" width="550" height="750" title="Select next" ></p></li>
<li><p>Accept the Cloudera license</p></li>
<li><p>Accept the Oracle binary license. The installer will proceed to install the Cloudera repository, gather files, and install the JDK followed by Cloudera Manager Server. Have patience as the installation can take some time.</p></li>
<li><p>When the installer finishes, it will indicate to open a web browser and navigate to the following url: <a href="http://localhost:7180/">http://localhost:7180/</a> The default username is <strong>admin</strong> and the password is <strong>admin</strong>. Open a web browser and use a url that will connect to the appropriate node in the cluster. After logging in, the installation will continue.</p>

<p> <img class="center" src="/images/cm/6_browser.PNG" width="550" height="750" title="Open a web browser" ></p></li>
<li><p>After signing in, the CM installer will displays a comparison of the different versions. Cloudera Express supports the goals of my project, if Enterprise is selected a trial will be provided for 60 days and then all features will return to the level of Cloudera Express. After reviewing the offerings, click your desired offering and then click continue.</p></li>
<li><p>CM displays the services available, click continue.</p></li>
<li><p>Now, all the nodes in the cluster will need to be selected. To select hosts, patterns are supported. Supported patters include</p>

<pre><code class="`"> 10.1.1.[1-4] -&gt; 10.1.1.1, 10.1.1.2, 10.1.1.3, 10.1.1.4
 host[1-3].network.com -&gt; host1.network.com, host2.network.com, host3.network.com
 host[07-10].network.com -&gt; host07.network.com, host08.network.com, host09.network.com, host10.network.com
</code></pre>

<p> My cluster has three hosts, and I chose to use IP address patterns. To select all the hosts I entered</p>

<pre><code class="`"> 10.1.0.[111-113]
</code></pre>

<p> If hosts are successfully found CM should display the following after clicking search</p>

<p> <img class="center" src="/images/cm/9_hosts.PNG" width="550" height="750" title="Hosts were found" ></p>

<p> If all the hosts were found, click continue, otherwise check the entered pattern/hostnames before continuing.</p></li>
<li><p>The CM installer will now confirm repository options, the defaults should be appropriate, so just click Continue.</p>

<p><img class="center" src="/images/cm/10_repo.PNG" width="550" height="750" title="Hosts were found" ></p></li>
<li><p>Next, the installer displays the Oracle license for the JDK. Click the check box at the bottom and then click Continue. Unless needed, the check box for Java Unlimited Strength Encryption Policy files does not need to be checked.</p></li>
<li><p>The installer will now ask if services should be installed in single user mode. To simplify the installation, <strong>do not</strong> click the check box and just click continue.</p></li>
<li><p>The installer will now need root SSH credentials. Ensure that root access via SSH on all hosts is possible and enter the appropriate credentials and port number. To simplify my own installation process, I have configured all root passwords in the cluster to be the same.</p>

<p><img class="center" src="/images/cm/13_ssh.PNG" width="550" height="750" title="Enter SSH info" ></p>

<p>After clicking continue, the installer will write needed files to all hosts in the cluster.</p></li>
<li><p>After the files have been written, the installer will indicate that operations have completed successfully. Click continue. If problems were encountered, an attempt to remove CM can be made and the installation restarted.</p>

<p><img class="center" src="/images/cm/14_cis.PNG" width="550" height="750" title="Cluster install successful" ></p></li>
<li><p>CM will now download, distribute and activate parcels on all hosts. After this process is complete, click Continue.</p>

<p><img class="center" src="/images/cm/15_parcel.PNG" width="550" height="750" title="Completed parcel distribution" ></p></li>
<li><p>Finally, CM will inspect all the hosts in the cluster. I do not recommend skipping this step. It will check for many common problems. If any are found, correct them. Otherwise, click finish to complete the CM installation.</p></li>
<li><p>Now CDH 5 services need to be selected. For my project I required HDFS, Hive, Hue (A great web interface for Hadoop administration) and YARN (Oozie will also be installed since Hue requires it). To install this subset of services, click the Custom Services radio button. Select the aforementioned services, and click Continue.</p></li>
<li><p>Cloudera Manager will next require that all roles in the cluster be delegated to nodes in the cluster. The master of the cluster should run all services except the HDFS DataNode and the YARN NodeManager. This would not be done in a production cluster as it would degrade performance of the services, but it is suitable for a test cluster. Ensure the master is also running the Cloudera Management Service, this was not selected to run on any nodes during my installation, but the ACtivity Monitor is a useful service to have running. After correctly delegating all the service to the nodes, click Continue.</p></li>
<li><p>The database needs to be configured for CM and CDH services. I prefer to use a MySQL database to maintain the necessary information. To configure MySQL for CM</p>

<p>Stop MySQL</p>

<pre><code>sudo service mysql stop
</code></pre>

<p>Edit <em>/etc/my.cnf</em></p>

<pre><code>sudo nano /etc/mysql/my.cnf
</code></pre>

<p>Change the file to the following (Keeping your own hardware restrictions in mind). Also, note max_connections, in a small cluster (&lt;50 nodes) this can be set to 250. In a larger cluster this can be set to 750.</p>

<pre><code>[mysqld]
transaction-isolation=READ-COMMITTED
# Disabling symbolic-links is recommended to prevent assorted security risks;
# to do so, uncomment this line:
# symbolic-links=0

key_buffer              = 16M
key_buffer_size         = 32M
max_allowed_packet      = 16M
thread_stack            = 256K
thread_cache_size       = 8
query_cache_limit       = 8M
query_cache_size        = 32M
query_cache_type        = 1

max_connections         = 250

# log-bin should be on a disk with enough free space
#log-bin=/x/home/mysql/logs/binary/mysql_binary_log

# For MySQL version 5.1.8 or later. Comment out binlog_format for older versions.
#binlog_format           = mixed

read_buffer_size = 2M
read_rnd_buffer_size = 16M
sort_buffer_size = 8M
join_buffer_size = 8M

# InnoDB settings
innodb_file_per_table = 1
innodb_flush_log_at_trx_commit  = 2
#innodb_log_buffer_size          = 50M

#256MB for every 2GB of RAM installed
innodb_buffer_pool_size         = 256M

#innodb_thread_concurrency = 2 * (cpu cores + disks)
innodb_thread_concurrency       = 4 

innodb_flush_method             = O_DIRECT
#innodb_log_file_size = 50M

[mysqld_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mysqld/mysqld.pid
</code></pre>

<p>Start MySQL</p>

<pre><code>sudo service mysql start
</code></pre>

<p>If MySQL fails to start, run the following for error indications
<code>
cat /var/log/mysql/error.log
</code></p>

<p>Install the MySQL JDBC connector</p>

<pre><code>sudo apt-get install libmysql-java
</code></pre>

<p>Now create the indicated databases and users (with the shown password) using the MySQL CLI or phpMyAdmin. Once everything has been created with the appropriate passwords, click Continue.</p></li>
<li></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing, Configuring and Testing Bacula]]></title>
    <link href="http://boldlycoding.com/blog/2014/12/26/installing-bacula/"/>
    <updated>2014-12-26T04:07:10-05:00</updated>
    <id>http://boldlycoding.com/blog/2014/12/26/installing-bacula</id>
    <content type="html"><![CDATA[<!-- more -->


<h2>Installation</h2>

<p>Cloudera Manager and Hadoop installations can be unstable so having backups of the file system becomes very important. Bacula create full, incremental, and differential backups of the filesystem. It support configuration of the backed up directory and exclusions to each backup. Bacula also allows restoration of previously created backups. To create these backups, restore files properly, and maintain other internal information, Bacula requires MySQL to be installed.</p>

<ol>
<li><p>To begin the install, issue the command:</p>

<pre><code class="`"> sudo apt-get install bacula-server bacula-client
</code></pre>

<p> To simply the install process, select Yes when asked if dbconfig-common should be used. This requires entering the database administrator password.</p></li>
<li><p>Create the directories to store Bacula&rsquo;s backup and restoration files</p>

<pre><code class="`"> sudo mkdir -p /bacula/backup /bacula/restore
</code></pre>

<p> Also, change permission so Bacula create correctly read and write.</p>

<pre><code class="`"> sudo chown -R bacula:bacula /bacula
 sudo chmod -R 700 /bacula
</code></pre></li>
<li><p>Now, Bacula must be configured to backup and restore to appropriate locations. To begin, the <em>bacula-dir.conf</em> file must be edited (The next three steps edit this file as well).</p>

<pre><code class="`"> sudo nano /etc/bacula/bacula-dir.conf
</code></pre>

<p> Within the <em>Job</em> under the comment block beginning <em>Standard Restore Template</em>, change the Where setting. To restore files in their original location it should be set to /. To restore them to the previously created directory, set it to <em>/bacula/restore</em>.</p>

<p> I configured my install to restore to the original locations</p>

<pre><code class="`"> Job {
   Name = "RestoreFiles"
   Type = Restore
   Client=Blank-fd
   FileSet="Full Set"
   Storage = File
   Pool = Default
   Messages = Standard
   Where = /
 }
</code></pre></li>
<li><p>To conserve disk space, I configured Bacula to use GZIP compression. This is found in the options section of the FileSet listed under the comment <em>List of files to be backed up</em></p>

<pre><code class="`"> Include {
     Options {
         signature = MD5
         compression = GZIP
     }
 }
</code></pre></li>
<li><p>Then set file to / to back up the entire file system. This option is found after the compression option under the next comment block.</p>

<pre><code class="`"> File = /
</code></pre></li>
<li><p>Now, configure Bacula to exclude its own file path. This is found in the exclude section, which should be just beyond the file option.</p>

<pre><code class="`"> Exclude {
     File = /var/lib/bacula
     File = /bacula
     File = /proc
     File = /tmp
     File = /.journal
     File = /.fsck
 }
</code></pre></li>
<li><p>Next, Bacula must be configured to back up the files described in the Job to the appropriate location. Backup locations are defined in <em>/etc/bacula/bacula-sd.conf</em></p>

<pre><code class="`"> sudo nano /etc/bacula/bacula-sd.conf
</code></pre>

<p> Under to comment block beginning <em>Devices supported by this Storage daemon</em>, change the Device group to</p>

<pre><code class="`"> Device {
   Name = FileStorage
   Media Type = File
   Archive Device = /bacula/backup
   LabelMedia = yes;                   # lets Bacula label unlabeled media
   Random Access = Yes;
   AutomaticMount = yes;               # when device opened, read it
   RemovableMedia = no;
   AlwaysOpen = no;
 }
</code></pre>

<p> This sets the backup location to the directory created in a previous step.</p></li>
<li><p>To validate the edited configuration, two command should be issued. If no errors are found, the command will return nothing.</p>

<pre><code class="`"> sudo bacula-dir -tc /etc/bacula/bacula-dir.conf
 sudo bacula-sd -tc /etc/bacula/bacula-sd.conf
</code></pre></li>
<li><p>Now, the Bacula services must be restarted to use the new configuration.</p>

<pre><code class="`"> sudo service bacula-sd restart
 sudo service bacula-director restart
</code></pre></li>
</ol>


<h2>Testing Bacula</h2>

<ol>
<li><p>Bacula backups and restores are started from Bacula&rsquo;s console, it can be started with</p>

<pre><code class="`"> sudo bconsole
</code></pre></li>
<li><p>To label the archive file created by the backup job that will be started in a later step use the command</p>

<pre><code class="`"> label
</code></pre>

<p> It will prompt for a name, something like Initial_Backup can be entered.</p></li>
<li><p>Next, the type of storage &ldquo;pool&rdquo;, to store the backup as a <em>File</em>, or archive select option 2 when prompted.</p></li>
<li><p>To begin the backup operation</p>

<pre><code class="`"> run
</code></pre>

<p> Select option 1, <em>BackupClient1</em>, to select the Job configured in the previous steps. This selects a full backup operation since this is the first backup made. If this job is done in the future, an incremental backup will be done. This can be reconfigured in the Job settings.</p></li>
<li><p>To confirm the selections and start the backup type</p>

<pre><code class="`"> yes
</code></pre>

<p> To view output generated by the job use the command</p>

<pre><code class="`"> messages
</code></pre>

<p> When the backup has finished, <em>Backup: OK</em> will be displayed. To exit the console type</p>

<pre><code class="`"> exit
</code></pre></li>
</ol>


<h2>Restoring Files</h2>

<p>When restoring files from a Bacula backup, a full backup must be restored before any incremental backups.</p>

<p>To list all the backup jobs that have been executed use from the Bacula console</p>

<pre><code>list jobs
</code></pre>

<ol>
<li><p>To restore all the files automatically type</p>

<pre><code class="`"> restore all
</code></pre></li>
<li><p>Then use option 5 to select the most recent backup. All files will be preselected from the most recent backup for restoration since <em>all</em> was used. If all is not specified, files can be selected from the tree for restoration.</p></li>
<li><p>To finish file selection type</p>

<pre><code class="`"> done
</code></pre></li>
<li><p>Like during the backup procedure the following can be typed to view generated output</p>

<pre><code class="`"> messages
</code></pre></li>
<li><p>When restoration has been completed type</p>

<pre><code class="`"> exit
</code></pre></li>
</ol>


<p>This covers a very basic restoration process. Documentation for the restore command exists here, for more advanced needs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing phpMyAdmin]]></title>
    <link href="http://boldlycoding.com/blog/2014/12/24/installing-phpmyadmin/"/>
    <updated>2014-12-24T13:30:41-05:00</updated>
    <id>http://boldlycoding.com/blog/2014/12/24/installing-phpmyadmin</id>
    <content type="html"><![CDATA[<!-- more -->


<p>phpMyAdmin simplies the task of database administration for all services running in the cluster. It is also incredibly useful when developing your own applications. Although the command line is still useful for certain DBA tasks, phpMyAdmin makes many faster and easier.</p>

<ol>
<li><p>To begin the installation issue the following command:</p>

<pre><code class="`"> sudo apt-get install phpmyadmin apache2-utils
</code></pre>

<p> Installing apache2-utils allows for the configuration of user restrictions if desired later.</p></li>
<li><p>When prompted, select apache2 to be automatically configured.</p></li>
<li><p>Next, select Yes when asked if dbconfig-common should configure the database. The administrative password for MySQL will be needed.</p></li>
<li><p>Navigate to:</p>

<pre><code class="`"> hostip/phpmyadmin
</code></pre>

<p> The installation should have configured Apache to serve phpMyAdmin. If for some reason phpMyAdmin was inaccessible, inspect:</p>

<pre><code class="`"> /etc/apache2/apache2.conf
</code></pre>

<p> and add, this line if needed</p>

<pre><code class="`"> Include /etc/phpmyadmin/apache.conf
</code></pre>

<p> Then restart apache</p>

<pre><code class="`"> Include /etc/phpmyadmin/apache.conf
</code></pre></li>
</ol>

]]></content>
  </entry>
  
</feed>
