<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big Data | To Boldly Code...]]></title>
  <link href="http://boldlycoding.com/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://boldlycoding.com/"/>
  <updated>2015-04-08T02:58:05-04:00</updated>
  <id>http://boldlycoding.com/</id>
  <author>
    <name><![CDATA[George McDaid]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Presenting the Tweets]]></title>
    <link href="http://boldlycoding.com/blog/2015/04/08/presenting-the-tweets/"/>
    <updated>2015-04-08T01:06:46-04:00</updated>
    <id>http://boldlycoding.com/blog/2015/04/08/presenting-the-tweets</id>
    <content type="html"><![CDATA[<!-- more -->


<p>After setting up the machines, installing the cluster software, configuring the services, writing the tweet collector, collecting the tweets and processing the tweets the time has finally come to present the information. Although the majority of this project has been very interesting, setting up the presentation was personally the most absorbing.</p>

<p>After processing the tweets the data available was the positive, negative, and neutral sentiment by country for a given topic. My largest dataset was the collection of tweets that contained the terms</p>

<pre><code>ArrayList&lt;String&gt; filter_trends = Lists.newArrayList("university", "school", "uni", "college", "education", "teacher", "teachers", "professor", "professors", "campus");
</code></pre>

<p>Since the tweets come from multiple countries the most appropriate medium for presentation is a map. Although bar and pie charts are appropriate, they become much more cumbersome to read as the number of countries increases. Since there are over 200 countries a map makes much more sense. There were several libraries that support data visualization on a map including Dygraphs, Leaflet, D3.js and Google Charts. Initially Google Charts seemed to be the best choice. It it implemented using HTML and JS and it every easy to use. It also generates what is called a choropleth  or thematic map. This expresses magnitude of data in a particular area. This also is where the problem occurs however. Choropleth maps only express magnitude, not proportion. To accurately represent the opinions in each area it is important that proportion be expressed with magnitude. Proportion cannot be represented on its own because although a particular sentiment may be dominating in an area, this may only be significant depending on the number of tweets. So, the expression of both magnitude and proportion becomes important. Unfortunatly, Google Charts only supports magnitude not proportion. After reviewing the other alternatives the Leaflet library soon emerged as the best candidate.  HumanGeo, an organization that works with big data, social media, and geospatial visualization, has developed a custom variant of Leaflet that caters extremely well to expressing data on a map called the Leaflet Data Visualization Framework or DVF. It supports a large variety of options including bar, radial bar, pie, cox comb, and stacked regular polygon charts. The layers can be stacked in any way desired and adorned with different colouring options and markers. After learning about all these options, Leaflet was committed to as the library to draw the map and express the data.</p>

<p>HumanGeo&rsquo;s Leaflet DVF, like many of the other frameworks, is implemented with HTML and JS. The HTML of the page is very basic, in includes a few DIVs as place holders for the map and legend. The rest is done by the JS.</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;title&gt;&lt;/title&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;meta name="description" content=""&gt;
    &lt;meta name="author" content=""&gt;

    &lt;link href="lib/bootstrap/css/bootstrap.css" rel="stylesheet"&gt;
    &lt;link rel="stylesheet" href="lib/bootstrap/css/bootstrap-responsive.css" &gt;
    &lt;link rel="stylesheet" href="http://cdn.leafletjs.com/leaflet-0.7.3/leaflet.css" type="text/css" media="screen" /&gt;
    &lt;link rel="stylesheet" href="dist/css/dvf.css" type="text/css" media="screen" /&gt;
    &lt;link rel="stylesheet" href="css/example.css" type="text/css" media="screen" /&gt;

&lt;/head&gt;

&lt;body&gt;


    &lt;div class="container-fluid"&gt;
        &lt;div class="row-fluid"&gt;
            &lt;div id="map" class="span9"&gt;&lt;/div&gt;
                        &lt;div id="legend" class="span3"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;script type="text/javascript" src="http://code.jquery.com/jquery-1.9.1.min.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="lib/bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="http://cdn.leafletjs.com/leaflet-0.7.3/leaflet.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="http://maps.stamen.com/js/tile.stamen.js?v1.2.3"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="lib/jsts/javascript.util.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="lib/jsts/jsts.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="lib/date.format.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="lib/geohash.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="dist/leaflet-dvf.min.js"&gt;&lt;/script&gt;

    &lt;script type="text/javascript"&gt;
        &lt;!-- Important Stuff Here --&gt;
    &lt;/script&gt;
    &lt;script type="text/javascript" src="dist/data/countryData.min.js"&gt;&lt;/script&gt;

&lt;/body&gt;
&lt;/html&gt; 
</code></pre>

<p>The HTML comment stating &ldquo;Important Stuff Here&rdquo; in a placeholder for what the remainder of this post will discuss. All of the map, data layer and legend generation occurs here. Since this is so significant, it warrants separation from the rest of the code.</p>

<p>&#8220;`</p>

<script type="text/javascript">
            
        <?php

        $sql = 
        "
        SELECT `country`, sentiment, total, `code`
        FROM `sets`
        JOIN `codes` USING (country)
        WHERE set_id = ?
        ORDER BY country ASC
        ";
        
        $stmt->free_result();
        $stmt = $conn->prepare($sql);
        $stmt->bind_param("s", $id);
        $stmt->execute();
        
        $stmt->bind_result($ccountry, $sent, $tot, $ccode);
        
        $country = "";
        $code = "";
        $out = 0;
        $pos = 0;
        $neg = 0;
        $neu = 0;
        ?>
            
        var telephoneLines = [
        
            <?php
                while($stmt->fetch()){
                    if($country != $ccountry && $country != ""){
            ?>                      "Total": "<?= $pos+$neg+$neu ?>",
                                    "CountryCode": "<?= $code ?>",
                                    "Country": "<?= $country ?>"
                            },
            <?php
                        $out = 0;
                    }

                    if($out == 0){
                       ?> { <?php
                    }
                    
                    if($sent == 2){
                        ?> "Positive": "<?= $tot ?>", <?php
                        $pos = $tot;
                    }else if($sent == 1){
                        ?> "Negative": "<?= $tot ?>", <?php
                        $neg = $tot;
                    }else if($sent == 0){
                        ?> "Neutral": "<?= $tot ?>", <?php
                        $neu = $tot;
                    }
            
                    $out++;
                    $country = $ccountry;
                    $code = $ccode;
            } ?>
           
           <?php 
           
           if($out > 0){
               ?>
                   "Total": "<?= $pos+$neg+$neu ?>",
                   "CountryCode": "<?= $code ?>",
                    "Country": "<?= $country ?>"
                },
               <?php
           }
           
          ?>
           
        ];  
                
         
        
        $(document).ready(function() {
    var map;
        
        var baseLayer = new L.StamenTileLayer('toner-lite', {
        detectRetina: true
    });

    var baseMaps = {
        "Stamen Toner": baseLayer
    };

    var categories = ['Positive','Negative','Neutral'];
        
        
        var red = new L.HSLHueFunction(new L.Point(1, 0), new L.Point(2, 0));

        var green = new L.HSLHueFunction(new L.Point(0, 120), new L.Point(1, 120));

        var blue = new L.HSLHueFunction(new L.Point(2, 240), new L.Point(3, 240));
        
        var fillColorFunctionBars = new L.PiecewiseFunction([green, red, blue]);
        
    var styleFunction = new L.StylesBuilder(categories,{
        displayName: function (index) {
            return categories[index];
        },
        color: 'hsl(0,100%,20%)',
        fillColor: fillColorFunctionBars,
        minValue: 0,
        maxValue:50000//300000000
    });

    options = {
                recordsField: null,
        locationMode: L.LocationModes.COUNTRY,
        codeField: 'CountryCode',
        chartOptions: styleFunction.getStyles(),
        layerOptions: {
            fillOpacity: 0.7,
            opacity: 1,
            width: 6,
                        color: '#000000',
                        weight: 1,
                        numberOfSides: 60,
                        rotation: 45
        },
        tooltipOptions: {
            iconSize: new L.Point(100,65),
            iconAnchor: new L.Point(-5,65)
        },

        onEachRecord: function (layer, record) {
            var $html = $(L.HTMLUtils.buildTable(record));

            layer.bindPopup($html.wrap('<div/>').parent().html(), {
                maxWidth: 400,
                minWidth: 400
            });
        }
    };

    //var telephoneLinesBarChart = new L.StackedRegularPolygonDataLayer(telephoneLines, options);
        var telephoneLinesBarChart = new L.PieChartDataLayer(telephoneLines, options);
        
        var nd = new L.HSLLuminosityFunction(new L.Point(-1, .06), new L.Point(1, .07), {outputHue: 75, outputSaturation: '100%'});
        var red = new L.HSLLuminosityFunction(new L.Point(1, 0.97), new L.Point(1000, 0.4), {outputHue: 305, outputSaturation: '100%'});
        var yellow = new L.HSLLuminosityFunction(new L.Point(1000, 0.85), new L.Point(100000, 0.3), {outputHue: 71, outputSaturation: '100%'});
        var blue = new L.HSLLuminosityFunction(new L.Point(100000, 0.85), new L.Point(1500000, 0.3), {outputHue: 200, outputSaturation: '100%'});
        var colorFunctions = new L.PiecewiseFunction([nd, red, yellow, blue]);
        
        var nd = new L.HSLLuminosityFunction(new L.Point(-1, .06), new L.Point(1, .07), {outputHue: 75, outputSaturation: '100%'});
        var red2 = new L.HSLLuminosityFunction(new L.Point(0, 0.97), new L.Point(1000, 0.4), {outputHue: 305, outputSaturation: '100%'});
        var yellow2 = new L.HSLLuminosityFunction(new L.Point(1000, 0.85), new L.Point(100000, 0.3), {outputHue: 71, outputSaturation: '100%'});
        var blue2 = new L.HSLLuminosityFunction(new L.Point(100000, 0.85), new L.Point(2000000, 0.3), {outputHue: 200, outputSaturation: '100%'});
        var fillColorFunctions = new L.PiecewiseFunction([nd, red, yellow, blue]);
        
//        var colorFunction = new L.HSLHueFunction(new L.Point(0,50), new L.Point(2000000,267), {outputSaturation: '100%', outputLuminosity: '30%'});
//  var fillColorFunction = new L.HSLHueFunction(new L.Point(0,50), new L.Point(2000000,267));

        var l1 = 1000;
        var l2 = 100000;
        var l3 = 2000000;
        
        var options = {
        recordsField: null,
        locationMode: L.LocationModes.COUNTRY,
        codeField: 'CountryCode',
                displayOptions: {
            Total: {
                displayName: 'Total: 0 to '+l1,
                                color: red,
                                fillColor: red2
            }
        },
                includeLayer: function (record) {
            return record['Total'] >= 0 && record['Total'] < l1; 
        },
        layerOptions: {
            fillOpacity: 0.8,
            opacity: 1,
            weight: 1,
            numberOfSides: 50
                        
        },
        tooltipOptions: {
            iconSize: new L.Point(80,60),
            iconAnchor: new L.Point(-5,55)
        }
    };
        
        tier1 = new L.ChoroplethDataLayer(telephoneLines,options);

        var options = {
        recordsField: null,
        locationMode: L.LocationModes.COUNTRY,
        codeField: 'CountryCode',
                displayOptions: {
            Total: {
                displayName: 'Total: '+l1+ " to "+l2,
                                color: yellow,
                                fillColor: yellow2
            }
        },
                includeLayer: function (record) {
            return record['Total'] >= l1 && record['Total'] < l2; 
        },
        layerOptions: {
            fillOpacity: 0.8,
            opacity: 1,
            weight: 1,
            numberOfSides: 50
                        
        },
        tooltipOptions: {
            iconSize: new L.Point(100,60),
            iconAnchor: new L.Point(-5,55)
        }
    };
        
        tier2 = new L.ChoroplethDataLayer(telephoneLines,options);
        
        var options = {
        recordsField: null,
        locationMode: L.LocationModes.COUNTRY,
        codeField: 'CountryCode',
                displayOptions: {
            Total: {
                displayName: 'Total: '+l2+ " to "+l3,
                                color: blue,
                                fillColor: blue2
            }
        },
                includeLayer: function (record) {
            return record['Total'] >= l2 && record['Total'] < l3; 
        },
        layerOptions: {
            fillOpacity: 0.8,
            opacity: 1,
            weight: 1,
            numberOfSides: 50
                        
        },
        tooltipOptions: {
            iconSize: new L.Point(100,70),
            iconAnchor: new L.Point(-5,65)
        }
    };
        
        tier3 = new L.ChoroplethDataLayer(telephoneLines,options);
        
    map = L.map('map', {
        layers: [baseLayer, tier1, tier2, tier3, telephoneLinesBarChart],
                zoomControl: false
    }).setView([13, 15], 3);
        
        $('#legend').append(tier1.getLegend({
                numSegments: 10,
                width: 160,
                className: 'well'
        }));
        
        $('#legend').append(tier2.getLegend({
                numSegments: 10,
                width: 160,
                className: 'well'
        }));
        
        $('#legend').append(tier3.getLegend({
                numSegments: 10,
                width: 160,
                className: 'well'
        }));
        
        // Disable drag and zoom handlers.
        map.dragging.disable();
        map.touchZoom.disable();
        map.doubleClickZoom.disable();
        map.scrollWheelZoom.disable();

        // Disable tap handler, if present.
        if (map.tap) map.tap.disable();
});
</script>


<pre><code>
It should first be noted that PHP code exists in the JS code. This allowed my pages to by dynamic and display any dataset imported into a MySQL database. A regular JS array can be placed here, this just prevents the page from being dynamic. The code begins by constructing the array containing the data. After the PHP code runs the array is output in the following format: 
</code></pre>

<p>[
    {<br/>
        &ldquo;Neutral&rdquo;: &ldquo;195&rdquo;,<br/>
        &ldquo;Negative&rdquo;: &ldquo;369&rdquo;,
        &ldquo;Positive&rdquo;: &ldquo;252&rdquo;,                     <br/>
        &ldquo;Total&rdquo;: &ldquo;816&rdquo;,
        &ldquo;CountryCode&rdquo;: &ldquo;AFG&rdquo;,
        &ldquo;Country&rdquo;: &ldquo;AFGHANISTAN&rdquo;
    },
    {<br/>
        &ldquo;Neutral&rdquo;: &ldquo;2643&rdquo;,<br/>
        &ldquo;Negative&rdquo;: &ldquo;4788&rdquo;,<br/>
        &ldquo;Positive&rdquo;: &ldquo;1398&rdquo;,                     <br/>
        &ldquo;Total&rdquo;: &ldquo;8829&rdquo;,
        &ldquo;CountryCode&rdquo;: &ldquo;ARG&rdquo;,
        &ldquo;Country&rdquo;: &ldquo;ARGENTINA&rdquo;
    },</p>

<pre><code>...
</code></pre>

<p>]
&#8220;`</p>

<p>The format I used has the records in the array with no other datasets. The entire array is a single dataset. With the DVF, it is possible to have multiple datasets in the data array. The data layers have a field called recordField, where this can be specified, If the data is the entire array, then null is passed. Moving forward, the code initializes the base map. A toner type map is used to provide high contrast. The DVF also supports watercolour world world maps. Next the code defines the categories for the data being presented which are the three categories of sentiment. After that, a very interesting portion of code is found, these are the colour mapping functions. These map the the scale of values to a colours scale. The scale of possible values if also broken up into tiers since there is a different value depending on the sentiment. This is supported with the piece wise function generator. At different values, different functions are used. This allows very accurate colour selection depending on the value. This function is passed to the style function for the pie chart layer. The style function is then passed to all of the options for the pie char layer. Notable options include the recordField (a null since there is only one dataset in the array), the locationField and codeField describe how countries are expressed in the dataset. Next, there are more colour declaration functions. These functions declare the colours for the countries that the chloropleth layers will colour. These functions are tiered to show the differences in magnitude between the countries since there is a great deal of variation in the total number of tweets from each country. From highest to lowest, they are coloured blue, yellow and then red. These functions of then passed to the appropriate data layer. One data layers is defined for each tier of values so a legend can easily be generated for each scale. Note that again, the recordField is null and the appropriate functions for the value tier are specified in colour and fillColor fields. Also, it is important that a conditional is found in the includeLayer field. This ensure on the data that is to be in that layer is included. After those three layers are defined, they are added to the map. The zoom is disabled to ensure the map stays centred in the view. Then, the scales are added to the legend. Finally, map movement and other zoom options are disabled to further ensure the map is cemented at all times.</p>

<p>When run, the HTML and JS together produce a fantastic visualization that really made the entire project worthwhile. The map with the chloropleth and pie charts can be viewed <a href="http://penguin.lhup.edu/~gmcdaid/uni.php?id=1" title="Fantastic Map Visualization">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Limitations and Problems With the Twitter Process]]></title>
    <link href="http://boldlycoding.com/blog/2015/03/18/limitations-and-problems-with-the-twitter-process/"/>
    <updated>2015-03-18T03:05:07-04:00</updated>
    <id>http://boldlycoding.com/blog/2015/03/18/limitations-and-problems-with-the-twitter-process</id>
    <content type="html"><![CDATA[<!-- more -->


<h2>Capturing Limitations</h2>

<p>There are a few limitations and problem with the currently implemented Twitter capturing and refinement process. The first problem is with the Flume Agent. When storing tweets, unless the configuration is modified they will be stored in thee same folder. This is not suitable if comparing the public sentiment on several topic. To remedy this situation the configuration must be changed for each new topic that tweets are captured for. In the future, it would be beneficial to in some way allow the Flume Twitter source to control which folder the tweets go into. The source could specify a different folder for each topic that is captured. Although, they source will also need to be modified to allow for capturing on several topic areas. Rate limits will need to considered, to ensure they are not violated, but when capturing on multiple topics it is best for several agents to run. Another limitation of the Twitter source is the filter terms. As implemented the filter terms can not adapt and do not learn. To adequately capture opinion on a topic all relevant search terms must be included. If an unforeseen filter term is omitted, a vast portion of the opinion on an issue could be unintentionally excluded. It would be more effective to analyse tweets and dynamically predict which terms should be tracked through use of a hash map or database structure.</p>

<h2>Processing Limitations</h2>

<p>After tweets have been captured, there are also a few problems and limitations with the processing methods. The largest limitation factor of the sentiment extraction process is the dictionary, or lexicon. The sentiment is going to be as accurate as the lexicon. The more correct values in the lexicon, the more accurate the sentiment is going to be. Society itself imposes problems on this. Language and society is constantly changing. Language constructs mean different things in different time periods and continue to evolve as we move into the future. The lexicon must also include terms for the topic in question. This, combined with changing language, means to accurately reflect sentiment the lexicon must be constantly maintained and revised. The size of this task aside, it is also critical to have access to this data, which is not the case for all individuals. Many datasets incur a cost that many cannot afford. This includes financial and technological, or computational costs. Computational costs are one more limitation of the process. To accurately describe sentiment on an issues, or topic, the processing power, disk space, memory capacity, and network bandwidth must all accommodate the volume of information needed. It become very difficult to create an environment where all of these factors completely cater to the current problem. During the course of this study memory in particular was a problem. It needed to accommodate all of the required services as well as the tweets in the Flume channel. Extensive configuration was required before a balance was found between all components that required memory.</p>

<p>There are many more limitations of the Twitter process that could be expounded upon, but these are the most pressing and interesting. In the future, progressive improvement could lead to a very powerful and impressive system.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Collecting and Processing Tweets]]></title>
    <link href="http://boldlycoding.com/blog/2015/03/17/collecting-and-processing-tweets/"/>
    <updated>2015-03-17T17:55:55-04:00</updated>
    <id>http://boldlycoding.com/blog/2015/03/17/collecting-and-processing-tweets</id>
    <content type="html"><![CDATA[<!-- more -->


<p><img class="center" src="/images/tweet_process.png" width="1000" height="400" title="Tweet collection and processing" ></p>

<h2>Collecting the Tweets</h2>

<p>Tweet collection is driven the Apache project Flume. Flume collects, aggregates, and moves variable amounts of streaming data. The data can be from a variety of sources including server and other logs. Flume overall is very flexible and customisable based on the particular application. It not only support multiple sources, but also multiple processing mediums, or channels. Here channels are where the data in question is stored before being being written to its final destination, or sink. Flume captures data from a source, and stores it in a channel before finally writing the data to the sink. Channels can be in memory, in a file, or files, or a combination of the two. The logical structure of the source, channel and sink, is an agent. A flume agent that captures web server data and writes it to a Hadoop HDFs would logically look like the following</p>

<p><img class="center" src="/images/flume_agent.png" width="540" height="440" title="A typical flume agent" ></p>

<p>As discussed in a previous post, I have installed Cloudera&rsquo;s CDH in my cluster. The CDH allows for convenient definition of Flume Agent. After selecting the Flume service and clicking the Configuration tab, property Configuration File can be searched for. My agent definition was</p>

<pre><code>TwitterAgent.sources = Twitter
TwitterAgent.channels = MemChannel
TwitterAgent.sinks = HDFS

TwitterAgent.sources.Twitter.type = com.boldlycoding.twitterstreaming.TrendingStream
TwitterAgent.sources.Twitter.channels = MemChannel

TwitterAgent.sinks.HDFS.channel = MemChannel
TwitterAgent.sinks.HDFS.type = hdfs
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://typhlosion:8020/user/flume/tweets/%Y/%m/%d/%H/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text
TwitterAgent.sinks.HDFS.hdfs.batchSize = 80
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
TwitterAgent.sinks.HDFS.hdfs.rollCount = 100
TwitterAgent.sinks.HDFS.hdfs.rollInterval = 120
TwitterAgent.sinks.HDFS.hdfs.minBlockReplicas = 1

TwitterAgent.channels.MemChannel.type = memory
TwitterAgent.channels.MemChannel.capacity = 10000
TwitterAgent.channels.MemChannel.transactionCapacity = 100
</code></pre>

<p>This Flume agent definition uses a custom flume source defined in my uploaded jar. The channel is a Memory Channel, meaning all the tweets will be placed in memory before being written to the sink, which here is the Hadoop HDFS. All of the tweets are written in a text format. The agent also writes 100 tweets to each file before starting a new file, and will automatically close a file after 2 minutes even if 100 tweets have not been written. The memory channel is defined with a capacity of 10,000 tweets and 100 tweets will be move in/out of the channel at a time. The HDFS path parameter defines where on the HDFS the files will be written and the percent signs and characters at the end of the path indicate the directories will be created based on the timestamp of each tweet.</p>

<p>To do the actual capturing, processing, and dispatching to the memory channel, I have defined a custom Flume source by extending the Flume abstract source, access credentials withheld.</p>

<pre><code>public class TrendingStream extends AbstractSource implements EventDrivenSource, Configurable {

    Authentication auth;

    @Override
    public void start() {
        final ChannelProcessor channel = getChannelProcessor();
        final BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;String&gt;(10000);
        StatusesFilterEndpoint endpoint = new StatusesFilterEndpoint();
        ArrayList&lt;String&gt; filter_trends = Lists.newArrayList("college", "university", "uni", "school", "higher education", "grad school");
        endpoint.trackTerms(filter_trends);
        Client client = new ClientBuilder()
                .hosts(Constants.STREAM_HOST)
                .endpoint(endpoint)
                .authentication(auth)
                .processor(new StringDelimitedProcessor(queue))
                .build();
        client.connect();
        final JsonParser jsonParser = new JsonParser();
        final Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;();

        TwitterThread tweam = new TwitterThread() {

            @Override
            public void run() {
                for (int msgRead = 0; msgRead &lt; 1000; msgRead++) {
                    try {
                        String msg = queue.take();
                        msg = msg.replace("\n", "").replace("\r", "").replace("\\n", "");
                        msg = msg.replaceAll("\\\\\"", "");
                        msg = msg.replaceAll("\\\\","");
                        msg = msg.replaceAll("/","");
                        //System.out.println(msg);
                        JsonObject user = jsonParser.parse(msg)
                                .getAsJsonObject().get("user")
                                .getAsJsonObject();
                        String zone = user.get("time_zone").getAsString();
                        //System.out.println(zone);
                        msg = msg.trim();
                        headers.put("timestamp", String.valueOf(new Date().getTime()));
                    Event event = EventBuilder.withBody(msg.getBytes(), headers);
                    channel.processEvent(event);
                    } catch (InterruptedException ex) {
                        Logger.getLogger(TrendingStream.class.getName()).log(Level.SEVERE, null, ex);
                    } catch (UnsupportedOperationException ex) {
                        msgRead--;
                    } catch (NullPointerException ex) {
                        msgRead--;
                        try {
                            Thread.sleep(2000);
                        } catch (InterruptedException ex1) {
                            System.out.println("Thread Interrupt");
                        }
                    }

                    if (msgRead &lt;= -10000) {
                        break;
                    }

                }
            }
        };

        tweam.run();

        super.start();

    }

    @Override
    public void configure(Context cntxt) {
        auth = new OAuth1("consumerKey", "consumerSecret", "token", "tokenSecret");
    }

    @Override
    public void stop() {
        super.stop();
    }
}
</code></pre>

<p>The source begins by defining a Flume channel processor, blocking queue, and a filtering status endpoint. The processor and queue and more obvious. The filtering status endpoint captures live tweets, and applies a set of filtering terms. A tweets must contain all the terms in one of the items in the set. For instance, referencing the set in the previous code snippet, a tweet with the work college would be captured. A tweet containing higher education would be captured as well, but a tweet only containing the word higher or education would not.</p>

<p>In this code snippet the loop would capture 1000 tweets before stopping. When a tweet is captured the code filters out backslashes, double backslashes, forward slashes, and new lines from the JSON data format of the tweet. This is important because if they are not filtered out it causes the JSON parser to throw and error and is unable to process the tweets. This was one of the more difficult points to discover, and filtering out all the necessary characters was an extremely iterative process. After the tweet is cleaned, the tweet is checked for a valid (non-blank) time zone which allows for regionalization of the sentiment data (It is interesting to note that since Twitter is built on Ruby all of the timezone options are the same as those that are found in Ruby). Through the course of the study I have noted that about half of tweets have an invalid time zone. If a valid time zone is not found, the loop is set back an iteration. This allows for the specified number of tweets to be captured. Also, to avoid an infinite loop the loop control variable is check for a large negative value, and a break is issued if a large value is found. This should be relative to the number of tweets that need to be captured. Finally, the tweet data is stamped with a timestamp and it is passed to the channel processor. The settings in the Flume Agent configuration then control how the channel handles placing the events into the sink. This was another very difficult part of the project to configure as the channel must be set up so it is not overwhelmed by the speed of incoming tweets.</p>

<h2>Processing the Tweets</h2>

<p>To process the sentiment data in the Hadoop HDFS Hive (provided with Cloudera CDH) is used. Hive is an Apache project that provides a SQL interface for querying the Hadoop HDFS. The run the SQL queries, Hive runs MapReduce jobs taking advantage of the other distributed Hadoop component. The following Hive script analyzes the Tweet data and produces several new tables the last of which contains the required sentiment data.</p>

<pre><code>DROP TABLE tweets_raw;
DROP TABLE dictionary;
DROP TABLE time_zone_map;
DROP VIEW tweets_simple;
DROP VIEW l1;
DROP VIEW l2;
DROP VIEW l3;
DROP TABLE tweets_sentiment;
DROP TABLE tweetsbi;

CREATE EXTERNAL TABLE tweets_raw (
   id BIGINT,
   created_at STRING,
   source STRING,
   favorited BOOLEAN,
   retweet_count INT,
   retweeted_status STRUCT&lt;
      text:STRING,
      user:STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
   entities STRUCT&lt;
      urls:ARRAY&lt;STRUCT&lt;expanded_url:STRING&gt;&gt;,
      user_mentions:ARRAY&lt;STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
      hashtags:ARRAY&lt;STRUCT&lt;text:STRING&gt;&gt;&gt;,
   text STRING,
   user STRUCT&lt;
      screen_name:STRING,
      name:STRING,
      friends_count:INT,
      followers_count:INT,
      statuses_count:INT,
      verified:BOOLEAN,
      utc_offset:STRING, -- was INT but nulls are strings
      time_zone:STRING&gt;
)
ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe'
LOCATION '/user/flume/tweets'
;

-- create sentiment dictionary
CREATE EXTERNAL TABLE dictionary (
    type string,
    length int,
    word string,
    pos string,
    stemmed string,
    polarity string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
STORED AS TEXTFILE
LOCATION '/user/home/gmcdaid/data/dictionary';

CREATE EXTERNAL TABLE time_zone_map (
    time_zone string,
    country string,
    notes string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
STORED AS TEXTFILE
LOCATION '/user/home/gmcdaid/data/time_zone_map';

CREATE VIEW tweets_simple AS
SELECT
  id,
  cast ( from_unixtime( unix_timestamp(concat( '2015 ', substring(created_at,5,15)), 'yyyy MMM dd hh:mm:ss')) as timestamp) ts,
  text,
  time_zone_map.country
FROM tweets_raw
JOIN time_zone_map ON tweets_raw.user.time_zone = time_zone_map.time_zone
;

create view l1 as select id, words from tweets_simple lateral view explode(sentences(lower(text))) dummy as words;
create view l2 as select id, word from l1 lateral view explode( words ) dummy as word ;

create view l3 as select 
    id, 
    l2.word, 
    case d.polarity 
      when  'negative' then -1
      when 'positive' then 1 
      else 0 end as polarity 
 from l2 left outer join dictionary d on l2.word = d.word;

create table tweets_sentiment stored as orc as select 
id, 
case 
when sum( polarity ) &gt; 0 then 'positive' 
when sum( polarity ) &lt; 0 then 'negative'  
else 'neutral' end as sentiment 
from l3 group by id;

CREATE TABLE tweetsbi 
STORED AS ORC
AS
SELECT 
  t.*,
  case s.sentiment 
    when 'positive' then 2 
    when 'neutral' then 1 
    when 'negative' then 0 
  end as sentiment  
FROM tweets_simple t JOIN tweets_sentiment s on t.id = s.id;
</code></pre>

<p>This script has a multitude of intermediate steps. It begins by creating a table containing all the raw Twitter information. The accommodate all the nested structures in the JSON Hive provides ARRAY and STRUCT structures. The table specifies a SERDE and LOCATION, the location leaves all the data on the file system instead of importing it. The SERDE clause instructs the table to parse the data using the specified serializer/deseralizer. This is a generic piece of code provided by Cloudera to process JSON data. Next, the script creates the dictionary and time zone tables. The dictionary contains all of the words to be analysed and their associated sentiment values. This is sometimes referred to as a lexicon. The time zone table maps the twitter time zones to generic regions. The list was created using the list of time zones that Ruby supports. After this, the script creates a simpler tweets table containing only the required information. Following this three lateral views are created to analyse the twitter data using the information from the dictionary file. The final two steps reorganize the data and create more readable tables. Positive sentiment becomes a 2, neutral sentiment is a 1, and negative sentiment is a 0. Following processing all of the tweet data, the data can be presented, in a later post I will discuss using PHP and Google charts to present the data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Twitter Trends]]></title>
    <link href="http://boldlycoding.com/blog/2015/02/11/twitter-trends/"/>
    <updated>2015-02-11T01:27:08-05:00</updated>
    <id>http://boldlycoding.com/blog/2015/02/11/twitter-trends</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Twitter defines trends as those topics which are popular now, not topics which have been popular for a long period of time. During the course of my project thus far I have been tracking trends and making observations about them. One interesting observation about Twitter trends is to volatility of the trends. Trends change on a frequent basis, which suggests an instability of conversation on Twitter. It is also interesting to note the
number of trends which are not of appreciable social, political or economic significance. From January to February 2015 trends included #ReplaceAMovieTitleWithGoat, #HappyBirthdayHarryStyles, #NationalPizzaDay, #RuinAFriendshipIn5Words and a number of other One Direction topics. The instability of trends and lack of politically, socially and economically trends does pose a problem for my project. To cope with this problem I have begun tracking trends every two minutes and reviewing the trends periodically. If a significant topic does begin to trends. I will be able to start tracking that trends and then analyse the related tweets. Twitter trends are also based on location. Twitter will tailor the trends displayed based on the location of the user. My project will focus on worldwide opinions and topics so this posed another temporary problem. The Twitter API allows for the acquisition of trends on a global scale. This list of trends is not influenced by any particular user&rsquo;s information or location. Overall, Trends tend to be very complex and will continue to shape the project towards it completion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing Cloudera Manager]]></title>
    <link href="http://boldlycoding.com/blog/2014/12/29/installing-cloudera-manager/"/>
    <updated>2014-12-29T08:47:51-05:00</updated>
    <id>http://boldlycoding.com/blog/2014/12/29/installing-cloudera-manager</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Cloudera Manager (CM) will install the Oracle JDK, CDH (runs Hadoop), and other service management software. The manager administrates many services including HDFS, YARN, ZooKeeper, Oozie, Hive, Hue, Sqoop, HBase, Impala, Solr and Spark. Many of these can be installed only if they are needed. The CM installation process includes a section to select all or a subset of the services.</p>

<ol>
<li><p>Start a tmux session with</p>

<pre><code class="`"> tmux
</code></pre>

<p> In the event of a disconnect reconnect to the machine and run</p>

<pre><code class="`"> tmux a
</code></pre>

<p> Tmux will try to attach to a previously running session.</p></li>
<li><p>Download and start the Cloudera Manager installer on the cluster manager</p>

<pre><code class="`"> wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin
 chmod u+x cloudera-manager-installer.bin
 sudo ./cloudera-manager-installer.bin
</code></pre></li>
<li><p>If the installation was started correctly, it should display what is shown in the screenshot below. Select <em>Next</em> to continue.</p>

<p> <img class="center" src="/images/cm/3_start.PNG" width="550" height="750" title="Select next" ></p></li>
<li><p>Accept the Cloudera license</p></li>
<li><p>Accept the Oracle binary license. The installer will proceed to install the Cloudera repository, gather files, and install the JDK followed by Cloudera Manager Server. Have patience as the installation can take some time.</p></li>
<li><p>When the installer finishes, it will indicate to open a web browser and navigate to the following url: <a href="http://localhost:7180/">http://localhost:7180/</a> The default username is <strong>admin</strong> and the password is <strong>admin</strong>. Open a web browser and use a url that will connect to the appropriate node in the cluster. After logging in, the installation will continue.</p>

<p> <img class="center" src="/images/cm/6_browser.PNG" width="550" height="750" title="Open a web browser" ></p></li>
<li><p>After signing in, the CM installer will displays a comparison of the different versions. Cloudera Express supports the goals of my project, if Enterprise is selected a trial will be provided for 60 days and then all features will return to the level of Cloudera Express. After reviewing the offerings, click your desired offering and then click continue.</p></li>
<li><p>CM displays the services available, click continue.</p></li>
<li><p>Now, all the nodes in the cluster will need to be selected. To select hosts, patterns are supported. Supported patters include</p>

<pre><code class="`"> 10.1.1.[1-4] -&gt; 10.1.1.1, 10.1.1.2, 10.1.1.3, 10.1.1.4
 host[1-3].network.com -&gt; host1.network.com, host2.network.com, host3.network.com
 host[07-10].network.com -&gt; host07.network.com, host08.network.com, host09.network.com, host10.network.com
</code></pre>

<p> My cluster has three hosts, and I chose to use IP address patterns. To select all the hosts I entered</p>

<pre><code class="`"> 10.1.0.[111-113]
</code></pre>

<p> If hosts are successfully found CM should display the following after clicking search</p>

<p> <img class="center" src="/images/cm/9_hosts.PNG" width="550" height="750" title="Hosts were found" ></p>

<p> If all the hosts were found, click continue, otherwise check the entered pattern/hostnames before continuing.</p></li>
<li><p>The CM installer will now confirm repository options, the defaults should be appropriate, so just click Continue.</p>

<p><img class="center" src="/images/cm/10_repo.PNG" width="550" height="750" title="Hosts were found" ></p></li>
<li><p>Next, the installer displays the Oracle license for the JDK. Click the check box at the bottom and then click Continue. Unless needed, the check box for Java Unlimited Strength Encryption Policy files does not need to be checked.</p></li>
<li><p>The installer will now ask if services should be installed in single user mode. To simplify the installation, <strong>do not</strong> click the check box and just click continue.</p></li>
<li><p>The installer will now need root SSH credentials. Ensure that root access via SSH on all hosts is possible and enter the appropriate credentials and port number. To simplify my own installation process, I have configured all root passwords in the cluster to be the same.</p>

<p><img class="center" src="/images/cm/13_ssh.PNG" width="550" height="750" title="Enter SSH info" ></p>

<p>After clicking continue, the installer will write needed files to all hosts in the cluster.</p></li>
<li><p>After the files have been written, the installer will indicate that operations have completed successfully. Click continue. If problems were encountered, an attempt to remove CM can be made and the installation restarted.</p>

<p><img class="center" src="/images/cm/14_cis.PNG" width="550" height="750" title="Cluster install successful" ></p></li>
<li><p>CM will now download, distribute and activate parcels on all hosts. After this process is complete, click Continue.</p>

<p><img class="center" src="/images/cm/15_parcel.PNG" width="550" height="750" title="Completed parcel distribution" ></p></li>
<li><p>Finally, CM will inspect all the hosts in the cluster. I do not recommend skipping this step. It will check for many common problems. If any are found, correct them. Otherwise, click finish to complete the CM installation.</p></li>
<li><p>Now CDH 5 services need to be selected. For my project I required HDFS, Hive, Hue (A great web interface for Hadoop administration) and YARN (Oozie will also be installed since Hue requires it). To install this subset of services, click the Custom Services radio button. Select the aforementioned services, and click Continue.</p></li>
<li><p>Cloudera Manager will next require that all roles in the cluster be delegated to nodes in the cluster. The master of the cluster should run all services except the HDFS DataNode and the YARN NodeManager. This would not be done in a production cluster as it would degrade performance of the services, but it is suitable for a test cluster. Ensure the master is also running the Cloudera Management Service, this was not selected to run on any nodes during my installation, but the ACtivity Monitor is a useful service to have running. After correctly delegating all the service to the nodes, click Continue.</p></li>
<li><p>The database needs to be configured for CM and CDH services. I prefer to use a MySQL database to maintain the necessary information. To configure MySQL for CM</p>

<p>Stop MySQL</p>

<pre><code>sudo service mysql stop
</code></pre>

<p>Edit <em>/etc/my.cnf</em></p>

<pre><code>sudo nano /etc/mysql/my.cnf
</code></pre>

<p>Change the file to the following (Keeping your own hardware restrictions in mind). Also, note max_connections, in a small cluster (&lt;50 nodes) this can be set to 250. In a larger cluster this can be set to 750.</p>

<pre><code>[mysqld]
transaction-isolation=READ-COMMITTED
# Disabling symbolic-links is recommended to prevent assorted security risks;
# to do so, uncomment this line:
# symbolic-links=0

key_buffer              = 16M
key_buffer_size         = 32M
max_allowed_packet      = 16M
thread_stack            = 256K
thread_cache_size       = 8
query_cache_limit       = 8M
query_cache_size        = 32M
query_cache_type        = 1

max_connections         = 250

# log-bin should be on a disk with enough free space
#log-bin=/x/home/mysql/logs/binary/mysql_binary_log

# For MySQL version 5.1.8 or later. Comment out binlog_format for older versions.
#binlog_format           = mixed

read_buffer_size = 2M
read_rnd_buffer_size = 16M
sort_buffer_size = 8M
join_buffer_size = 8M

# InnoDB settings
innodb_file_per_table = 1
innodb_flush_log_at_trx_commit  = 2
#innodb_log_buffer_size          = 50M

#256MB for every 2GB of RAM installed
innodb_buffer_pool_size         = 256M

#innodb_thread_concurrency = 2 * (cpu cores + disks)
innodb_thread_concurrency       = 4 

innodb_flush_method             = O_DIRECT
#innodb_log_file_size = 50M

[mysqld_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mysqld/mysqld.pid
</code></pre>

<p>Start MySQL</p>

<pre><code>sudo service mysql start
</code></pre>

<p>If MySQL fails to start, run the following for error indications
<code>
cat /var/log/mysql/error.log
</code></p>

<p>Install the MySQL JDBC connector</p>

<pre><code>sudo apt-get install libmysql-java
</code></pre>

<p>Now create the indicated databases and users (with the shown password) using the MySQL CLI or phpMyAdmin. Once everything has been created with the appropriate passwords, click Continue.</p></li>
<li></li>
</ol>

]]></content>
  </entry>
  
</feed>
